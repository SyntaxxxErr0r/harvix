24.03	Kullback%E2%80%93Leibler_divergence	Kullback–Leibler divergence	In probability theory and information theory, the Kullback–Leibler divergence (also information divergence, information gain, relative entropy, or KLIC) is a non-symmetric measure of the difference between two probability distributions P and Q. KL measures the expected number of extra bits required to code samples from P when using a code based on Q, rather than using a code based on P. Typically P represents the "true" distribution of data, observations, or a precisely calculated theoretical distribution. The measure Q typically represents a theory, model, description, or approximation of P.	http://upload.wikimedia.org/wikipedia/en/thumb/a/a8/KL-Gauss-Example.png/320px-KL-Gauss-Example.png
